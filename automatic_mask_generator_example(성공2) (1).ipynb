{"cells":[{"cell_type":"code","execution_count":1,"id":"Li7EtPINu3VU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21691,"status":"ok","timestamp":1725244069963,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"Li7EtPINu3VU","outputId":"a3529111-ee75-414e-dd25-762cbd8bb11a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-02 02:27:27--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.70, 13.227.219.59, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   132MB/s    in 21s     \n","\n","2024-09-02 02:27:48 (118 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n"]}],"source":["!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n"]},{"cell_type":"code","execution_count":2,"id":"5fa21d44","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7274,"status":"ok","timestamp":1725244198448,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"5fa21d44","outputId":"e1eafba2-cfe6-4861-c0d2-0ba75d4c550e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-gds1fk27\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-gds1fk27\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: segment_anything\n","  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segment_anything: filename=segment_anything-1.0-py3-none-any.whl size=36590 sha256=6cd48c6872740a4bd6a97cb4302ee190f9fdae23783df70c113dcb23acf2e2ec\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hif5d3pj/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n","Successfully built segment_anything\n","Installing collected packages: segment_anything\n","Successfully installed segment_anything-1.0\n"]}],"source":["!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n"]},{"cell_type":"markdown","id":"b7c0041e","metadata":{"id":"b7c0041e"},"source":["# Automatically generating object masks with SAM"]},{"cell_type":"markdown","id":"289bb0b4","metadata":{"id":"289bb0b4"},"source":["Since SAM can efficiently process prompts, masks for the entire image can be generated by sampling a large number of prompts over an image. This method was used to generate the dataset SA-1B.\n","\n","The class `SamAutomaticMaskGenerator` implements this capability. It works by sampling single-point input prompts in a grid over the image, from each of which SAM can predict multiple masks. Then, masks are filtered for quality and deduplicated using non-maximal suppression. Additional options allow for further improvement of mask quality and quantity, such as running prediction on multiple crops of the image or postprocessing masks to remove small disconnected regions and holes."]},{"cell_type":"code","execution_count":null,"id":"072e25b8","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":40},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1725108389955,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"072e25b8","outputId":"c19ac744-d192-4a57-e018-6b10420ff024"},"outputs":[{"data":{"text/html":["\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import display, HTML\n","display(HTML(\n","\"\"\"\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\"\"\"\n","))"]},{"cell_type":"markdown","id":"c0b71431","metadata":{"id":"c0b71431"},"source":["## Environment Set-up"]},{"cell_type":"markdown","id":"47e5a78f","metadata":{"id":"47e5a78f"},"source":["If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."]},{"cell_type":"code","execution_count":3,"id":"4fe300fb","metadata":{"id":"4fe300fb","executionInfo":{"status":"ok","timestamp":1725244212928,"user_tz":-540,"elapsed":730,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}}},"outputs":[],"source":["using_colab = True"]},{"cell_type":"code","execution_count":4,"id":"0685a2f5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23524,"status":"ok","timestamp":1725244239372,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"0685a2f5","outputId":"d215657f-0de9-4856-f65e-38bb7340759d"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version: 2.4.0+cu121\n","Torchvision version: 0.19.0+cu121\n","CUDA is available: True\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-i3czloq0\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-i3czloq0\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","--2024-09-02 02:30:27--  https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 99846 (98K) [image/jpeg]\n","Saving to: ‘images/dog.jpg’\n","\n","dog.jpg             100%[===================>]  97.51K  --.-KB/s    in 0.008s  \n","\n","2024-09-02 02:30:27 (12.1 MB/s) - ‘images/dog.jpg’ saved [99846/99846]\n","\n","--2024-09-02 02:30:27--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.59, 13.227.219.70, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth.1’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   228MB/s    in 10s     \n","\n","2024-09-02 02:30:38 (240 MB/s) - ‘sam_vit_h_4b8939.pth.1’ saved [2564550879/2564550879]\n","\n"]}],"source":["if using_colab:\n","    import torch\n","    import torchvision\n","    print(\"PyTorch version:\", torch.__version__)\n","    print(\"Torchvision version:\", torchvision.__version__)\n","    print(\"CUDA is available:\", torch.cuda.is_available())\n","    import sys\n","    !{sys.executable} -m pip install opencv-python matplotlib\n","    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n","\n","    !mkdir images\n","    !wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n","\n","    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"]},{"cell_type":"markdown","id":"fd2bc687","metadata":{"id":"fd2bc687"},"source":["## Set-up"]},{"cell_type":"code","execution_count":5,"id":"560725a2","metadata":{"id":"560725a2","executionInfo":{"status":"ok","timestamp":1725244254416,"user_tz":-540,"elapsed":1074,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"code","execution_count":6,"id":"74b6e5f0","metadata":{"id":"74b6e5f0","executionInfo":{"status":"ok","timestamp":1725244256369,"user_tz":-540,"elapsed":522,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}}},"outputs":[],"source":["def show_anns(anns):\n","    if len(anns) == 0:\n","        return\n","    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n","    ax = plt.gca()\n","    ax.set_autoscale_on(False)\n","\n","    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n","    img[:,:,3] = 0\n","    for ann in sorted_anns:\n","        m = ann['segmentation']\n","        color_mask = np.concatenate([np.random.random(3), [0.35]])\n","        img[m] = color_mask\n","    ax.imshow(img)"]},{"cell_type":"markdown","id":"27c41445","metadata":{"id":"27c41445"},"source":["## Example image"]},{"cell_type":"code","execution_count":7,"id":"ad354922","metadata":{"id":"ad354922","executionInfo":{"status":"ok","timestamp":1725244267356,"user_tz":-540,"elapsed":470,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}}},"outputs":[],"source":["image = cv2.imread('/content/sample_data/naver_map3.jpg')\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"]},{"cell_type":"code","execution_count":8,"id":"e0ac8c67","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447,"output_embedded_package_id":"1V35S_7laCsj6A2lFihRuRJrKihRwIiaU"},"executionInfo":{"elapsed":10819,"status":"ok","timestamp":1725244280925,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"e0ac8c67","outputId":"00e6de70-f9b0-47fa-c7b4-1e3eacf5f0d1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","id":"b8c2824a","metadata":{"id":"b8c2824a"},"source":["## Automatic mask generation"]},{"cell_type":"markdown","id":"d9ef74c5","metadata":{"id":"d9ef74c5"},"source":["To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."]},{"cell_type":"code","execution_count":9,"id":"1848a108","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19283,"status":"ok","timestamp":1725244304165,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"1848a108","outputId":"99d2fd8f-67d8-4a34-c5a0-b8955fb27466"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]}],"source":["import sys\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)"]},{"cell_type":"markdown","id":"d6b1ea21","metadata":{"id":"d6b1ea21"},"source":["To generate masks, just run `generate` on an image."]},{"cell_type":"code","execution_count":10,"id":"391771c1","metadata":{"executionInfo":{"elapsed":12207,"status":"ok","timestamp":1725244673378,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"391771c1"},"outputs":[],"source":["masks = mask_generator.generate(image)"]},{"cell_type":"markdown","id":"e36a1a39","metadata":{"id":"e36a1a39"},"source":["Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n","* `segmentation` : the mask\n","* `area` : the area of the mask in pixels\n","* `bbox` : the boundary box of the mask in XYWH format\n","* `predicted_iou` : the model's own prediction for the quality of the mask\n","* `point_coords` : the sampled input point that generated this mask\n","* `stability_score` : an additional measure of mask quality\n","* `crop_box` : the crop of the image used to generate this mask in XYWH format"]},{"cell_type":"code","execution_count":11,"id":"4fae8d66","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":463,"status":"ok","timestamp":1725244971624,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"4fae8d66","outputId":"1b72f1c7-fc20-4847-b8d0-8781e4689b8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["111\n","dict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\n"]}],"source":["print(len(masks))\n","print(masks[0].keys())"]},{"cell_type":"markdown","id":"53009a1f","metadata":{"id":"53009a1f"},"source":["Show all the masks overlayed on the image."]},{"cell_type":"code","execution_count":12,"id":"77ac29c5","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447,"output_embedded_package_id":"15FeG3nCYnCnL44lfyM7YKVHvxPeuywCQ"},"executionInfo":{"elapsed":14321,"status":"ok","timestamp":1725244988225,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"77ac29c5","outputId":"f122945f-60db-409c-ee7b-770b8a7d35b0","scrolled":false},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(masks)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":13,"id":"zx-pt4VHT6o0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":6366,"status":"error","timestamp":1725245014175,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"zx-pt4VHT6o0","outputId":"97a1b680-6121-4f04-e5af-b95a1b67d70a"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-06fdea147d26>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_model_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msam_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py\u001b[0m in \u001b[0;36mbuild_sam_vit_h\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_sam_vit_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     return _build_sam(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mencoder_embed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mencoder_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py\u001b[0m in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1098\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m     \u001b[0;31m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1455\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverall_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstorage_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstorage_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m         \u001b[0;31m# swap here if byteswapping is needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbyteorderdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests  # 여기에 requests 모듈 임포트 추가\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","\n","    return predicted_class.item()\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for mask in masks:\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    class_id = classify_masked_region(image_np, mask_np)\n","    label = imagenet_labels[class_id]\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), label, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"d_kyx_GChkIY","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":600,"output_embedded_package_id":"1q_jWcUM7mBQf-9xk-ZXfFtMymm5NLzBH"},"executionInfo":{"elapsed":177132,"status":"ok","timestamp":1725109439685,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"d_kyx_GChkIY","outputId":"bf7823de-17aa-4cd8-df20-ccf49fc5d62d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    # 필요한 경우 다른 레이블도 매핑\n","}\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    return custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for mask in masks:\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), label, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"_N9XTuFqoHfC","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ZKZeZliOPRhHH_8bkHg0LBR3LpbkWb9O"},"executionInfo":{"elapsed":172700,"status":"ok","timestamp":1725111172061,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"_N9XTuFqoHfC","outputId":"447fd1f8-3d19-407f-881d-ca522f06fa1c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    # 필요한 경우 다른 레이블도 매핑\n","}\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    return custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for mask in masks:\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), label, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","execution_count":null,"id":"6LCQUu7ntOdB","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"129nCH1zMbMoNDgaJN75kxXzfne00oQOH"},"executionInfo":{"elapsed":180878,"status":"ok","timestamp":1725112499587,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"6LCQUu7ntOdB","outputId":"9c6af935-1b18-4fd1-fb51-b15918fbd660"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    # 필요한 경우 다른 레이블도 매핑\n","}\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    return custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","execution_count":null,"id":"OvDo5KgZvJbY","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1_grWUNH9V0P7FzdmkiWYgva_30mlubh9"},"executionInfo":{"elapsed":181921,"status":"ok","timestamp":1725113004277,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"OvDo5KgZvJbY","outputId":"68e8b562-781f-4182-f1cf-e4d756eb732a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑 (오류 발생 시 교정)\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    # 필요한 경우 다른 레이블도 매핑\n","}\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","    # 신뢰도 검토 후 \"Unknown\"으로 설정 (선택사항)\n","    # if logit_value < some_threshold:\n","    #     corrected_label = \"Unknown\"\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","execution_count":null,"id":"Pk8SFA9kyc4N","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17YpWLgHaMVr-mxr-nJmOvMTnT5cKaFCt"},"executionInfo":{"elapsed":176798,"status":"ok","timestamp":1725113872799,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"Pk8SFA9kyc4N","outputId":"a7d29a56-05e6-49b4-a652-49ae2cd03a0f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑 (오류 발생 시 교정)\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    # 필요한 경우 다른 레이블도 추가 매핑 가능\n","}\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","execution_count":null,"id":"R7igB_i70wSa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TUNWouzbaBf2xCLqqXGGImCvujZ5CmqJ"},"executionInfo":{"elapsed":182652,"status":"ok","timestamp":1725114497380,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"R7igB_i70wSa","outputId":"5372b482-0538-4840-f79e-850354487de8"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import os\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# JSON 파일 경로\n","custom_labels_file = 'custom_labels.json'\n","\n","# 초기 레이블링 규칙 (최초 실행 시 사용)\n","initial_custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    # 필요한 경우 다른 레이블도 추가 매핑 가능\n","}\n","\n","# JSON 파일로 저장하는 함수\n","def save_custom_labels(custom_labels, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(custom_labels, f, indent=4)\n","\n","# JSON 파일에서 불러오는 함수\n","def load_custom_labels(file_path):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    else:\n","        return {}\n","\n","# 기존 규칙을 파일로 저장 (최초 실행 시)\n","if not os.path.exists(custom_labels_file):\n","    save_custom_labels(initial_custom_labels, custom_labels_file)\n","\n","# 코드 실행 시 JSON 파일에서 레이블링 규칙 로드\n","custom_labels = load_custom_labels(custom_labels_file)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 로드된 커스텀 레이블로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","execution_count":15,"id":"EcPUvSck3YYX","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1dE9-EzizzHfumpxBrj97Lxl3613hcSR1","height":1000},"id":"EcPUvSck3YYX","outputId":"8fb5bc5b-60f0-4733-ff88-689de39a4203","executionInfo":{"status":"ok","timestamp":1725245612957,"user_tz":-540,"elapsed":91147,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import os\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map3.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# JSON 파일 경로\n","custom_labels_file = 'custom_labels.json'\n","\n","# 초기 레이블링 규칙 (최초 실행 시 사용)\n","initial_custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    \"match\": \"building\",            # match를 building으로 매핑\n","    \"microphone\": \"building\",       # microphone을 building으로 매핑\n","}\n","\n","# JSON 파일로 저장하는 함수\n","def save_custom_labels(custom_labels, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(custom_labels, f, indent=4)\n","\n","# JSON 파일에서 불러오는 함수\n","def load_custom_labels(file_path):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    else:\n","        return {}\n","\n","# 기존 규칙을 파일로 저장 (최초 실행 시)\n","if not os.path.exists(custom_labels_file):\n","    save_custom_labels(initial_custom_labels, custom_labels_file)\n","\n","# 코드 실행 시 JSON 파일에서 레이블링 규칙 로드\n","custom_labels = load_custom_labels(custom_labels_file)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 로드된 커스텀 레이블로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import os\n","from shapely.geometry import Polygon, Point, LineString\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map3.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# JSON 파일 경로\n","custom_labels_file = 'custom_labels.json'\n","\n","# 초기 레이블링 규칙 (최초 실행 시 사용)\n","initial_custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    \"match\": \"building\",            # match를 building으로 매핑\n","    \"microphone\": \"building\",       # microphone을 building으로 매핑\n","}\n","\n","# JSON 파일로 저장하는 함수\n","def save_custom_labels(custom_labels, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(custom_labels, f, indent=4)\n","\n","# JSON 파일에서 불러오는 함수\n","def load_custom_labels(file_path):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    else:\n","        return {}\n","\n","# 기존 규칙을 파일로 저장 (최초 실행 시)\n","if not os.path.exists(custom_labels_file):\n","    save_custom_labels(initial_custom_labels, custom_labels_file)\n","\n","# 코드 실행 시 JSON 파일에서 레이블링 규칙 로드\n","custom_labels = load_custom_labels(custom_labels_file)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 로드된 커스텀 레이블로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","# 장애물 다각형을 저장할 리스트 초기화\n","obstacle_polygons = []\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","    # 장애물 다각형 추가\n","    obstacle_polygons.append(Polygon(zip(x, y)))\n","\n","# RRT 알고리즘 파라미터 설정\n","max_iter = 1000\n","step_size = 10\n","\n","# 시작점과 도착점 임의 지정 (이미지 크기에 따라 적절히 조정)\n","start = np.array([100, 100])\n","goal = np.array([400, 400])\n","\n","def is_collision_free(p1, p2, obstacles):\n","    \"\"\"p1과 p2를 연결하는 선이 장애물과 충돌하는지 확인\"\"\"\n","    line = LineString([p1, p2])\n","    return all(not line.intersects(obstacle) for obstacle in obstacles)\n","\n","def rrt(start, goal, obstacles, max_iter=1000, step_size=10):\n","    \"\"\"RRT 알고리즘 구현\"\"\"\n","    nodes = [start]\n","    for _ in range(max_iter):\n","        # 무작위 샘플링\n","        rand_point = np.random.rand(2) * np.array(image_np.shape[1::-1])\n","        # 가장 가까운 노드 찾기\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - rand_point))\n","        direction = (rand_point - nearest_node) / np.linalg.norm(rand_point - nearest_node)\n","        new_node = nearest_node + direction * step_size\n","        new_node = np.clip(new_node, 0, np.array(image_np.shape[1::-1]) - 1)\n","\n","        if is_collision_free(nearest_node, new_node, obstacles):\n","            nodes.append(new_node)\n","            # 목표 지점 근처에 도달하면 종료\n","            if np.linalg.norm(new_node - goal) < step_size:\n","                nodes.append(goal)\n","                break\n","\n","    # 경로 생성\n","    path = [goal]\n","    while np.linalg.norm(path[-1] - start) > step_size:\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - path[-1]))\n","        path.append(nearest_node)\n","    path.append(start)\n","    return path[::-1]\n","\n","# RRT로 경로 찾기\n","path = rrt(start, goal, obstacle_polygons, max_iter=max_iter, step_size=step_size)\n","\n","# 경로 시각화\n","plt.plot([p[0] for p in path], [p[1] for p in path], '-o', color='blue', markersize=5)\n","plt.scatter(*start, color='green', s=100, label='Start')\n","plt.scatter(*goal, color='red', s=100, label='Goal')\n","\n","# 장애물 시각화\n","for obstacle in obstacle_polygons:\n","    x, y = obstacle.exterior.xy\n","    plt.fill(x, y, color='gray', alpha=0.5)\n","\n","plt.axis('off')\n","plt.legend()\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uwSCFX35678WvStJ675ip2WS_KDf9jWY"},"id":"Hf1kk3q3rO3C","executionInfo":{"status":"ok","timestamp":1725246287494,"user_tz":-540,"elapsed":267276,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}},"outputId":"daef4854-f385-4309-82be-fc9e809a35ac"},"id":"Hf1kk3q3rO3C","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import os\n","from shapely.geometry import Polygon, Point, LineString\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map3.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# JSON 파일 경로\n","custom_labels_file = 'custom_labels.json'\n","\n","# 초기 레이블링 규칙 (최초 실행 시 사용)\n","initial_custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    \"match\": \"building\",            # match를 building으로 매핑\n","    \"microphone\": \"building\",       # microphone을 building으로 매핑\n","}\n","\n","# JSON 파일로 저장하는 함수\n","def save_custom_labels(custom_labels, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(custom_labels, f, indent=4)\n","\n","# JSON 파일에서 불러오는 함수\n","def load_custom_labels(file_path):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    else:\n","        return {}\n","\n","# 기존 규칙을 파일로 저장 (최초 실행 시)\n","if not os.path.exists(custom_labels_file):\n","    save_custom_labels(initial_custom_labels, custom_labels_file)\n","\n","# 코드 실행 시 JSON 파일에서 레이블링 규칙 로드\n","custom_labels = load_custom_labels(custom_labels_file)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 로드된 커스텀 레이블로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","# 장애물 다각형을 저장할 리스트 초기화\n","obstacle_polygons = []\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","    # 장애물 다각형 추가\n","    obstacle_polygons.append(Polygon(zip(x, y)))\n","\n","# RRT 알고리즘 파라미터 설정\n","max_iter = 1000\n","step_size = 10\n","\n","# 임의의 시작점과 도착점을 10개 생성 (이미지 크기에 따라 적절히 조정)\n","num_paths = 10\n","start_goal_pairs = []\n","for _ in range(num_paths):\n","    start = np.random.randint(0, image_np.shape[1], 2)\n","    goal = np.random.randint(0, image_np.shape[1], 2)\n","    start_goal_pairs.append((start, goal))\n","\n","def is_collision_free(p1, p2, obstacles):\n","    \"\"\"p1과 p2를 연결하는 선이 장애물과 충돌하는지 확인\"\"\"\n","    line = LineString([p1, p2])\n","    return all(not line.intersects(obstacle) for obstacle in obstacles)\n","\n","def rrt(start, goal, obstacles, max_iter=1000, step_size=10):\n","    \"\"\"RRT 알고리즘 구현\"\"\"\n","    nodes = [start]\n","    for _ in range(max_iter):\n","        # 무작위 샘플링\n","        rand_point = np.random.rand(2) * np.array(image_np.shape[1::-1])\n","        # 가장 가까운 노드 찾기\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - rand_point))\n","        direction = (rand_point - nearest_node) / np.linalg.norm(rand_point - nearest_node)\n","        new_node = nearest_node + direction * step_size\n","        new_node = np.clip(new_node, 0, np.array(image_np.shape[1::-1]) - 1)\n","\n","        if is_collision_free(nearest_node, new_node, obstacles):\n","            nodes.append(new_node)\n","            # 목표 지점 근처에 도달하면 종료\n","            if np.linalg.norm(new_node - goal) < step_size:\n","                nodes.append(goal)\n","                break\n","\n","    # 경로 생성\n","    path = [goal]\n","    while np.linalg.norm(path[-1] - start) > step_size:\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - path[-1]))\n","        path.append(nearest_node)\n","    path.append(start)\n","    return path[::-1]\n","\n","# 모든 경로 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for start, goal in start_goal_pairs:\n","    # RRT로 경로 찾기\n","    path = rrt(start, goal, obstacle_polygons, max_iter=max_iter, step_size=step_size)\n","\n","    # 경로 시각화\n","    plt.plot([p[0] for p in path], [p[1] for p in path], '-o', markersize=5)\n","    plt.scatter(*start, color='green', s=100, label='Start')\n","    plt.scatter(*goal, color='red', s=100, label='Goal')\n","\n","# 장애물 시각화\n","for obstacle in obstacle_polygons:\n","    x, y = obstacle.exterior.xy\n","    plt.fill(x, y, color='gray', alpha=0.5)\n","\n","plt.legend()\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"],"metadata":{"id":"V51iVXJUu6Z8"},"id":"V51iVXJUu6Z8","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GGg1kwAAu7h-"},"id":"GGg1kwAAu7h-","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":17,"id":"9VeJ-0fRioBB","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"executionInfo":{"elapsed":19830,"status":"error","timestamp":1725247002192,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"},"user_tz":-540},"id":"9VeJ-0fRioBB","outputId":"5741ad65-1b3e-4483-cabd-b9ce29dbdbbc"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/sample_data/naver_map.jpg'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-6812a196858e>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 이미지 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample_data/naver_map.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/naver_map.jpg'"]}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import random\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map3.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# 임시 클래스 매핑\n","custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    # 필요한 경우 다른 레이블도 매핑\n","}\n","\n","# 랜덤 색상 생성 함수\n","def random_color():\n","    return [random.randint(0, 255) for _ in range(3)]\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    # 마스크된 객체 영역만 잘라내기\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","\n","    # 전처리 후 모델에 입력\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    # 분류 결과 얻기\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 커스텀 라벨로 매핑\n","    return custom_labels.get(original_label, original_label)  # 매핑된 값이 없으면 원래 레이블 반환\n","\n","# 이미지 초기화\n","output_image = image_np.copy()\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","\n","for mask in masks:\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가\n","    y, x = np.where(mask_np)\n","    color = random_color()  # 랜덤 색상 선택\n","    output_image[mask_np] = color  # 객체 영역에 색상 적용\n","\n","    plt.text(x.min(), y.min(), label, color='white', fontsize=12, bbox=dict(facecolor='black', alpha=0.6))\n","\n","# 이미지 출력\n","plt.imshow(output_image)\n","plt.axis('off')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"56dmNN9ulXTj","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"56dmNN9ulXTj","outputId":"b3b39440-92c5-49c6-8946-2cc5e79bd5dc"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(f)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Generated 111 masks.\n","dict_keys(['segmentation', 'area', 'bbox', 'predicted_iou', 'point_coords', 'stability_score', 'crop_box'])\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-68b7bfd66ba8>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_goal_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# RRT로 경로 찾기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobstacle_polygons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# 경로 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-68b7bfd66ba8>\u001b[0m in \u001b[0;36mrrt\u001b[0;34m(start, goal, obstacles, max_iter, step_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mnearest_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearest_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-68b7bfd66ba8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mnearest_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnearest_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_imag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_imag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2552\u001b[0;31m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2553\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import sys\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import models, transforms\n","import requests\n","import json\n","import os\n","from shapely.geometry import Polygon, Point, LineString\n","\n","sys.path.append(\"..\")\n","from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n","\n","# SAM 모델 설정\n","sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n","model_type = \"vit_h\"\n","device = \"cuda\"\n","\n","sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n","sam.to(device=device)\n","\n","mask_generator = SamAutomaticMaskGenerator(sam)\n","\n","# 이미지 로드\n","image = Image.open(\"/content/sample_data/naver_map3.jpg\").convert(\"RGB\")\n","image_np = np.array(image)\n","\n","# 마스크 생성\n","masks = mask_generator.generate(image_np)\n","print(f\"Generated {len(masks)} masks.\")\n","print(masks[0].keys())\n","\n","# 객체 분류를 위한 이미지 분류 모델 로드 (여기서는 ResNet 사용)\n","classifier = models.resnet50(pretrained=True)\n","classifier.eval()\n","classifier.to(device)\n","\n","# 이미지 전처리\n","preprocess = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# ImageNet 클래스 레이블 로드\n","LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n","response = requests.get(LABELS_URL)\n","imagenet_labels = json.loads(response.text)\n","\n","# JSON 파일 경로\n","custom_labels_file = 'custom_labels.json'\n","\n","# 초기 레이블링 규칙 (최초 실행 시 사용)\n","initial_custom_labels = {\n","    \"spotlight\": \"building\",\n","    \"street_sign\": \"road\",\n","    \"parking_meter\": \"car\",\n","    \"cleaver\": \"industrial\",\n","    \"tennis ball\": \"industrial\",\n","    \"ping-pong ball\": \"industrial\",\n","    \"nematode\": \"building\",         # 잘못된 레이블을 building으로 매핑\n","    \"syringe\": \"building\",          # 잘못된 레이블을 building으로 매핑\n","    \"home theater\": \"building\",     # 잘못된 레이블을 building으로 매핑\n","    \"match\": \"building\",            # match를 building으로 매핑\n","    \"microphone\": \"building\",       # microphone을 building으로 매핑\n","}\n","\n","# JSON 파일로 저장하는 함수\n","def save_custom_labels(custom_labels, file_path):\n","    with open(file_path, 'w') as f:\n","        json.dump(custom_labels, f, indent=4)\n","\n","# JSON 파일에서 불러오는 함수\n","def load_custom_labels(file_path):\n","    if os.path.exists(file_path):\n","        with open(file_path, 'r') as f:\n","            return json.load(f)\n","    else:\n","        return {}\n","\n","# 기존 규칙을 파일로 저장 (최초 실행 시)\n","if not os.path.exists(custom_labels_file):\n","    save_custom_labels(initial_custom_labels, custom_labels_file)\n","\n","# 코드 실행 시 JSON 파일에서 레이블링 규칙 로드\n","custom_labels = load_custom_labels(custom_labels_file)\n","\n","# 객체 분리 및 분류\n","def classify_masked_region(image, mask):\n","    masked_image = Image.fromarray(image * mask[..., np.newaxis])\n","    input_tensor = preprocess(masked_image).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = classifier(input_tensor)\n","\n","    _, predicted_class = output.max(1)\n","    original_label = imagenet_labels[predicted_class.item()]\n","\n","    # 로드된 커스텀 레이블로 매핑\n","    corrected_label = custom_labels.get(original_label, original_label)\n","\n","    return corrected_label\n","\n","# 라벨 이름을 저장할 리스트 초기화\n","detected_labels = []\n","\n","# 객체 분리 및 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","# 장애물 다각형을 저장할 리스트 초기화\n","obstacle_polygons = []\n","\n","for i, mask in enumerate(masks, 1):\n","    mask_np = mask['segmentation']\n","\n","    # 마스크된 객체를 분류\n","    label = classify_masked_region(image_np, mask_np)\n","\n","    # 라벨 이름 저장\n","    detected_labels.append(label)\n","\n","    # 객체 경계 그리기 및 텍스트 레이블 추가 (번호 포함)\n","    y, x = np.where(mask_np)\n","    plt.text(x.min(), y.min(), f\"{i}: {label}\", color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.6))\n","    plt.contour(mask_np, colors='red', linewidths=1)\n","\n","    # 장애물 다각형 추가\n","    obstacle_polygons.append(Polygon(zip(x, y)))\n","\n","# RRT 알고리즘 파라미터 설정\n","max_iter = 1000\n","step_size = 10\n","\n","# 임의의 시작점과 도착점을 10개 생성 (이미지 크기에 따라 적절히 조정)\n","num_paths = 10\n","start_goal_pairs = []\n","for _ in range(num_paths):\n","    start = np.random.randint(0, image_np.shape[1], 2)\n","    goal = np.random.randint(0, image_np.shape[1], 2)\n","    start_goal_pairs.append((start, goal))\n","\n","def is_collision_free(p1, p2, obstacles):\n","    \"\"\"p1과 p2를 연결하는 선이 장애물과 충돌하는지 확인\"\"\"\n","    line = LineString([p1, p2])\n","    return all(not line.intersects(obstacle) for obstacle in obstacles)\n","\n","def rrt(start, goal, obstacles, max_iter=1000, step_size=10):\n","    \"\"\"RRT 알고리즘 구현\"\"\"\n","    nodes = [start]\n","    for _ in range(max_iter):\n","        # 무작위 샘플링\n","        rand_point = np.random.rand(2) * np.array(image_np.shape[1::-1])\n","        # 가장 가까운 노드 찾기\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - rand_point))\n","        direction = (rand_point - nearest_node) / np.linalg.norm(rand_point - nearest_node)\n","        new_node = nearest_node + direction * step_size\n","        new_node = np.clip(new_node, 0, np.array(image_np.shape[1::-1]) - 1)\n","\n","        if is_collision_free(nearest_node, new_node, obstacles):\n","            nodes.append(new_node)\n","            # 목표 지점 근처에 도달하면 종료\n","            if np.linalg.norm(new_node - goal) < step_size:\n","                nodes.append(goal)\n","                break\n","\n","    # 경로 생성\n","    path = [goal]\n","    while np.linalg.norm(path[-1] - start) > step_size:\n","        nearest_node = min(nodes, key=lambda n: np.linalg.norm(n - path[-1]))\n","        path.append(nearest_node)\n","    path.append(start)\n","    return path[::-1]\n","\n","# 모든 경로 시각화\n","plt.figure(figsize=(20,20))\n","plt.imshow(image_np)\n","plt.axis('off')\n","\n","for start, goal in start_goal_pairs:\n","    # RRT로 경로 찾기\n","    path = rrt(start, goal, obstacle_polygons, max_iter=max_iter, step_size=step_size)\n","\n","    # 경로 시각화\n","    plt.plot([p[0] for p in path], [p[1] for p in path], '-o', markersize=5)\n","    plt.scatter(*start, color='green', s=100, label='Start')\n","    plt.scatter(*goal, color='red', s=100, label='Goal')\n","\n","# 장애물 시각화\n","for obstacle in obstacle_polygons:\n","    x, y = obstacle.exterior.xy\n","    plt.fill(x, y, color='gray', alpha=0.5)\n","\n","plt.legend()\n","plt.show()\n","\n","# 모든 검출된 라벨 출력\n","print(\"Detected Labels:\")\n","for i, label in enumerate(detected_labels, 1):\n","    print(f\"{i}: {label}\")\n"]},{"cell_type":"code","source":["! ps -ef | grep python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jv1n53smwG96","executionInfo":{"status":"ok","timestamp":1725247291308,"user_tz":-540,"elapsed":463,"user":{"displayName":"안동철/학생/컴퓨터공학","userId":"13711896771637427813"}},"outputId":"aa7850cc-2314-412c-f5d0-e38d5d025f4e"},"id":"jv1n53smwG96","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["root          57       7  0 02:17 ?        00:00:06 [python3] <defunct>\n","root          58       7  0 02:17 ?        00:00:00 python3 /usr/local/bin/colab-fileshim.py\n","root         107       7  0 02:17 ?        00:00:05 /usr/bin/python3 /usr/local/bin/jupyter-notebook\n","root        2372     107 11 02:27 ?        00:06:26 /usr/bin/python3 -m colab_kernel_launcher -f /ro\n","root        2421       1  0 02:27 ?        00:00:07 /usr/bin/python3 /usr/local/lib/python3.10/dist-\n","root       16000    2372  0 03:21 ?        00:00:00 /bin/bash -c  ps -ef | grep python\n","root       16002   16000  0 03:21 ?        00:00:00 grep python\n"]}]},{"cell_type":"code","source":["! kill -9 {107}"],"metadata":{"id":"x0l8paPDwU_c"},"id":"x0l8paPDwU_c","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"00b3d6b2","metadata":{"id":"00b3d6b2"},"source":["## Automatic mask generation options"]},{"cell_type":"markdown","id":"183de84e","metadata":{"id":"183de84e"},"source":["There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:"]},{"cell_type":"code","execution_count":null,"id":"68364513","metadata":{"id":"68364513"},"outputs":[],"source":["mask_generator_2 = SamAutomaticMaskGenerator(\n","    model=sam,\n","    points_per_side=32,\n","    pred_iou_thresh=0.86,\n","    stability_score_thresh=0.92,\n","    crop_n_layers=1,\n","    crop_n_points_downscale_factor=2,\n","    min_mask_region_area=100,  # Requires open-cv to run post-processing\n",")"]},{"cell_type":"code","execution_count":null,"id":"bebcdaf1","metadata":{"id":"bebcdaf1"},"outputs":[],"source":["masks2 = mask_generator_2.generate(image)"]},{"cell_type":"code","execution_count":null,"id":"b8473f3c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1724997154386,"user":{"displayName":"안동철","userId":"14855697511434657289"},"user_tz":-540},"id":"b8473f3c","outputId":"88228b35-bb66-4ee8-812b-383aebbf5347"},"outputs":[{"data":{"text/plain":["679"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(masks2)"]},{"cell_type":"code","execution_count":null,"id":"fb702ae3","metadata":{"id":"fb702ae3"},"outputs":[],"source":["plt.figure(figsize=(20,20))\n","plt.imshow(image)\n","show_anns(masks2)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"8c937160","metadata":{"id":"8c937160"},"outputs":[],"source":["plt.figure(figsize=(20, 20))\n","\n","# 흑백으로 이미지를 표시하기 위해 cmap='gray'를 추가합니다.\n","plt.imshow(image, cmap='gray')\n","show_anns(masks2)\n","plt.axis('off')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/facebookresearch/segment-anything/blob/main/notebooks/automatic_mask_generator_example.ipynb","timestamp":1724994719710}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":5}